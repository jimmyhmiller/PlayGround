use std::{
    collections::{HashMap, HashSet},
    fs::{self, File},
    path::{Path, PathBuf},
    process::Command,
    sync::{Arc, Mutex},
    time::{Duration, Instant},
};

use anyhow::{Context, Result, anyhow, ensure};
use chrono::{TimeZone, Utc};
use gix::{
    ObjectId, Repository,
    bstr::{BString, ByteSlice},
    objs::tree::EntryKind,
    prelude::ObjectIdExt,
};
use rayon::{ThreadPool, prelude::*};
use serde_json::json;

// Type alias for easier migration - gix uses ObjectId instead of Oid
type Oid = ObjectId;

mod filetypes;
mod filter;
mod gix_compat;

use filter::FileFilter;
use gix_compat::oid_to_string;

/// Performance timing metrics
#[derive(Debug, Default)]
struct TimingStats {
    tree_walk_time: Duration,
    blame_time: Duration,
    hunk_processing_time: Duration,
    other_time: Duration,
}

impl TimingStats {
    fn print_report(&self, total_time: Duration) {
        let accounted = self.tree_walk_time + self.blame_time + self.hunk_processing_time;
        let other = if total_time > accounted {
            total_time - accounted
        } else {
            Duration::ZERO
        };

        eprintln!("\n=== PERFORMANCE BREAKDOWN ===");
        eprintln!("Total time:           {:>8.2?}", total_time);
        eprintln!("Tree walking:         {:>8.2?} ({:>5.1}%)",
            self.tree_walk_time,
            self.tree_walk_time.as_secs_f64() / total_time.as_secs_f64() * 100.0);
        eprintln!("Git blame calls:      {:>8.2?} ({:>5.1}%)",
            self.blame_time,
            self.blame_time.as_secs_f64() / total_time.as_secs_f64() * 100.0);
        eprintln!("Processing hunks:     {:>8.2?} ({:>5.1}%)",
            self.hunk_processing_time,
            self.hunk_processing_time.as_secs_f64() / total_time.as_secs_f64() * 100.0);
        eprintln!("Other operations:     {:>8.2?} ({:>5.1}%)",
            other,
            other.as_secs_f64() / total_time.as_secs_f64() * 100.0);
        eprintln!("=============================\n");
    }
}

/// Context for a worker thread containing pre-loaded git objects to avoid repeated allocations
struct WorkerContext {
    repo: Repository,
    ignore_whitespace: bool,
    use_mailmap: bool,
    mailmap: Option<gix::mailmap::Snapshot>,
    timing: Arc<Mutex<TimingStats>>,
    commit_graph: Option<std::sync::Arc<gix::commitgraph::Graph>>,
}

#[derive(Debug, Clone)]
pub struct AnalyzeConfig {
    pub repo: PathBuf,
    pub cohort_format: String,
    pub interval_secs: u64,
    pub ignore_patterns: Vec<String>,
    pub only_patterns: Vec<String>,
    pub outdir: PathBuf,
    pub branch: String,
    pub all_filetypes: bool,
    pub ignore_whitespace: bool,
    pub quiet: bool,
    pub jobs: usize,
    pub opt: bool,
}

impl AnalyzeConfig {
    pub fn validate(self) -> Result<Self> {
        ensure!(
            self.repo.exists(),
            "Repository path {:?} does not exist",
            self.repo
        );
        ensure!(
            self.repo.join(".git").exists(),
            "Repository {:?} does not look like a git repository (missing .git)",
            self.repo
        );
        Ok(self)
    }
}

pub fn analyze(config: AnalyzeConfig) -> Result<()> {
    let config = config.validate()?;
    perform_analysis(config)
}

#[allow(dead_code)]
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
enum CurveKeyKind {
    Cohort,
    Author,
    Domain,
    Ext,
    Dir,
    Sha,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct CurveKey {
    kind: CurveKeyKind,
    label: String,
}

impl CurveKey {
    fn new(kind: CurveKeyKind, label: String) -> Self {
        Self { kind, label }
    }

    fn cohort(label: String) -> Self {
        Self::new(CurveKeyKind::Cohort, label)
    }

    fn author(label: String) -> Self {
        Self::new(CurveKeyKind::Author, label)
    }

    fn domain(label: String) -> Self {
        Self::new(CurveKeyKind::Domain, label)
    }

    fn extension(label: String) -> Self {
        Self::new(CurveKeyKind::Ext, label)
    }

    fn directory(label: String) -> Self {
        Self::new(CurveKeyKind::Dir, label)
    }

    fn sha(label: String) -> Self {
        Self::new(CurveKeyKind::Sha, label)
    }

    fn kind(&self) -> CurveKeyKind {
        self.kind
    }

    fn label(&self) -> &str {
        &self.label
    }
}

#[allow(dead_code)]
#[derive(Debug)]
struct MasterSample {
    commit_id: Oid,
    committed_time: i64,
}

#[allow(dead_code)]
#[derive(Debug, Clone)]
struct FileEntry {
    path: String,
    blob_id: Oid,
}

fn perform_analysis(config: AnalyzeConfig) -> Result<()> {
    let repo = gix::open(&config.repo).with_context(|| {
        format!(
            "Unable to open repository at {}",
            config.repo.to_string_lossy()
        )
    })?;

    if config.opt {
        run_commit_graph(&config.repo, config.quiet)
            .context("Running git commit-graph optimization")?;
    }

    // Load mailmap if available
    let mailmap = Some(repo.open_mailmap());
    let (branch_name, branch_ref_name) =
        resolve_branch(&repo, &config.branch, config.quiet).context("Resolving branch")?;

    // Walk commits using gix revision API
    let mut commit2cohort_map: HashMap<Oid, String> = HashMap::new();
    let mut curve_keys: HashSet<CurveKey> = HashSet::new();

    // Get the reference and traverse commits
    let reference = repo.find_reference(&branch_ref_name)?;
    let peeled = reference.into_fully_peeled_id()?.detach();

    // Traverse all commits from the branch
    let commit_iter = peeled.ancestors(&repo.objects);

    for commit_result in commit_iter {
        let commit_info = commit_result?;
        let commit_id = commit_info.id;
        let commit_obj = repo.find_object(commit_id)?;
        let commit = commit_obj.try_into_commit()?;

        let time_seconds = commit.time()?.seconds;
        let cohort = format_cohort(time_seconds, &config.cohort_format)?;
        curve_keys.insert(CurveKey::cohort(cohort.clone()));
        commit2cohort_map.insert(commit_id, cohort);

        let (author, domain) = resolve_author(&commit, mailmap.as_ref());
        curve_keys.insert(CurveKey::author(author));
        curve_keys.insert(CurveKey::domain(domain));
    }

    eprintln!("Traversed {} commits and built commit2cohort map", commit2cohort_map.len());

    // Get head commit for sampling
    let head_ref = repo.find_reference(&branch_ref_name)?;
    let head_id = head_ref.into_fully_peeled_id()?;
    let head_obj = repo.find_object(head_id.detach())?;
    let head_commit = head_obj.try_into_commit()
        .context("Unable to resolve head commit for branch")?;

    let master_samples = collect_master_commits(&repo, head_id.detach(), config.interval_secs as i64)?;

    let filter = FileFilter::new(
        config.all_filetypes,
        &config.only_patterns,
        &config.ignore_patterns,
    )?;

    let mut commit_entries: Vec<Vec<FileEntry>> = Vec::new();
    for sample in &master_samples {
        let commit_obj = repo.find_object(sample.commit_id)?;
        let commit = commit_obj.try_into_commit()
            .with_context(|| format!("Unable to load commit {}", sample.commit_id))?;
        let entries = collect_commit_entries(&repo, &commit, &filter, &mut curve_keys)?;
        commit_entries.push(entries);
    }

    let commit2cohort = Arc::new(commit2cohort_map);
    let use_mailmap = mailmap.is_some();

    if !config.quiet {
        let total_files: usize = commit_entries.iter().map(|entries| entries.len()).sum();
        eprintln!(
            "Collected {} commits on branch '{}' ({} sampled commits for analysis); tracking {} files",
            commit2cohort.len(),
            branch_name,
            master_samples.len(),
            total_files
        );
    }

    let timing_stats = Arc::new(Mutex::new(TimingStats::default()));
    let start_time = Instant::now();

    let result = compute_curves(
        &repo,
        &config.repo,
        &commit_entries,
        &mut curve_keys,
        Arc::clone(&commit2cohort),
        &master_samples,
        use_mailmap,
        &config,
        Arc::clone(&timing_stats),
    );

    let total_time = start_time.elapsed();
    timing_stats.lock().unwrap().print_report(total_time);

    result
}

fn resolve_branch(repo: &Repository, requested: &str, quiet: bool) -> Result<(String, String)> {
    let requested_ref = format!("refs/heads/{requested}");
    match repo.find_reference(&requested_ref) {
        Ok(reference) => {
            let name = reference.name().as_bstr().to_string();
            Ok((requested.to_string(), name))
        }
        Err(_) => {
            let head = repo.head().context("Unable to resolve HEAD")?;
            let head_ref = head.try_into_referent()
                .ok_or_else(|| anyhow!("HEAD is detached; please specify a branch"))?;
            let fallback_name = head_ref.name()
                .shorten()
                .to_string();
            if !quiet {
                eprintln!(
                    "Requested branch '{}' does not exist. Falling back to default branch '{}'",
                    requested, fallback_name
                );
            }
            let name = head_ref.name().as_bstr().to_string();
            Ok((fallback_name, name))
        }
    }
}

fn format_cohort(seconds: i64, format: &str) -> Result<String> {
    let dt = Utc
        .timestamp_opt(seconds, 0)
        .single()
        .ok_or_else(|| anyhow!("Invalid timestamp"))?;
    Ok(dt.format(format).to_string())
}

fn resolve_author(commit: &gix::Commit<'_>, mailmap: Option<&gix::mailmap::Snapshot>) -> (String, String) {
    let commit_ref = commit.decode().expect("Failed to decode commit");
    let signature = commit_ref.author();
    let resolved = if let Some(map) = mailmap {
        map.resolve(signature).to_owned()
    } else {
        signature.to_owned().expect("Failed to parse signature")
    };

    let name = resolved.name.to_string();
    let email = resolved.email.to_string();
    let domain = email.split('@').nth(1).unwrap_or("").to_string();
    (name, domain)
}

fn compute_curves(
    repo: &Repository,
    repo_path: &Path,
    commit_entries: &[Vec<FileEntry>],
    curve_keys: &mut HashSet<CurveKey>,
    commit2cohort: Arc<HashMap<Oid, String>>,
    master_samples: &[MasterSample],
    use_mailmap: bool,
    config: &AnalyzeConfig,
    timing: Arc<Mutex<TimingStats>>,
) -> Result<()> {
    // Auto-detect CPU count if jobs is 0, but cap at 4 to avoid excessive git contention
    // Git operations (especially blame) don't scale well beyond 4-6 threads
    let jobs = if config.jobs == 0 {
        std::thread::available_parallelism()
            .map(|n| usize::min(n.get(), 4))
            .unwrap_or(2)
    } else {
        usize::max(1, config.jobs)
    };

    let pool = if jobs > 1 {
        Some(
            rayon::ThreadPoolBuilder::new()
                .num_threads(jobs)
                .build()
                .context("Failed to create rayon thread pool")?,
        )
    } else {
        None
    };

    let mut last_file_y: HashMap<String, HashMap<CurveKey, i64>> = HashMap::new();
    let mut cur_y: HashMap<CurveKey, i64> = HashMap::new();
    let mut curves: HashMap<CurveKey, Vec<i64>> = HashMap::new();
    let mut ts: Vec<String> = Vec::new();
    let mut commit_history: HashMap<String, Vec<(i64, i64)>> = HashMap::new();
    let mut last_file_hash: HashMap<String, Oid> = HashMap::new();

    for (idx, sample) in master_samples.iter().enumerate() {
        let commit_obj = repo.find_object(sample.commit_id)?;
        let commit = commit_obj.try_into_commit()
            .with_context(|| format!("Unable to reload commit {}", sample.commit_id))?;
        let commit_id = sample.commit_id;
        let entries = &commit_entries[idx];

        let mut cur_file_hash: HashMap<String, Oid> = HashMap::new();
        let mut check_entries: Vec<FileEntry> = Vec::new();

        for entry in entries {
            cur_file_hash.insert(entry.path.clone(), entry.blob_id);
            match last_file_hash.get(&entry.path) {
                Some(prev) if prev == &entry.blob_id => {}
                Some(_) => {
                    if let Some(prev_map) = last_file_y.get(&entry.path) {
                        for (key, count) in prev_map {
                            *cur_y.entry(key.clone()).or_insert(0) -= count;
                        }
                    }
                    check_entries.push(entry.clone());
                }
                None => {
                    check_entries.push(entry.clone());
                }
            };
        }

        // Remove deleted files directly without collecting into Vec
        last_file_hash.retain(|deleted_path, _| {
            if cur_file_hash.contains_key(deleted_path) {
                true // Keep this file
            } else {
                // File was deleted, subtract its counts
                if let Some(prev_map) = last_file_y.remove(deleted_path) {
                    for (key, count) in prev_map {
                        *cur_y.entry(key).or_insert(0) -= count;
                    }
                }
                false // Remove this file
            }
        });

        last_file_hash = cur_file_hash;

        let blame_results = run_blame_for_entries(
            repo_path,
            commit_id,
            &check_entries,
            Arc::clone(&commit2cohort),
            config.ignore_whitespace,
            use_mailmap,
            jobs,
            pool.as_ref(),
            Arc::clone(&timing),
        )?;

        for (path, file_histogram) in blame_results {
            for key in file_histogram.keys() {
                curve_keys.insert(key.clone());
            }
            for (key, count) in &file_histogram {
                *cur_y.entry(key.clone()).or_insert(0) += *count;
            }
            last_file_y.insert(path, file_histogram);
        }

        let timestamp_secs = commit.time()?.seconds;
        ts.push(format_timestamp(timestamp_secs)?);

        for key in curve_keys.iter() {
            let value = *cur_y.get(key).unwrap_or(&0);
            curves.entry(key.clone()).or_default().push(value);
        }

        update_commit_history(&mut commit_history, &cur_y, timestamp_secs);
    }

    write_outputs(curve_keys, &curves, &ts, &commit_history, config)
}

fn collect_master_commits(repo: &Repository, head_id: ObjectId, interval_secs: i64) -> Result<Vec<MasterSample>> {
    let mut samples = Vec::new();
    let mut last_timestamp: Option<i64> = None;

    // Walk commits from head
    let commit_iter = head_id.ancestors(&repo.objects);

    for commit_result in commit_iter {
        let commit_info = commit_result?;
        let commit_obj = repo.find_object(commit_info.id)?;
        let commit = commit_obj.try_into_commit()?;

        let ts = commit.time()?.seconds;
        if last_timestamp.is_none() || ts < last_timestamp.unwrap() - interval_secs {
            samples.push(MasterSample {
                commit_id: commit_info.id,
                committed_time: ts,
            });
            last_timestamp = Some(ts);
        }
    }

    samples.reverse();
    Ok(samples)
}

fn run_commit_graph(repo_path: &Path, _quiet: bool) -> Result<()> {
    let status = Command::new("git")
        .arg("commit-graph")
        .arg("write")
        .arg("--changed-paths")
        .current_dir(repo_path)
        .status()
        .context("Failed to invoke git commit-graph")?;

    if !status.success() {
        let code = status.code().unwrap_or(-1);
        return Err(anyhow!(
            "git commit-graph exited with status {}; see stderr for details",
            code
        ));
    }
    Ok(())
}

fn collect_commit_entries(
    repo: &Repository,
    commit: &gix::Commit<'_>,
    filter: &FileFilter,
    curve_keys: &mut HashSet<CurveKey>,
) -> Result<Vec<FileEntry>> {
    let tree = commit.tree()?;

    let mut entries = Vec::new();

    // Traverse tree recursively - need to decode to get TreeRef
    let tree_ref = tree.decode()?;
    traverse_tree(repo, &tree_ref, "", filter, curve_keys, &mut entries)?;

    Ok(entries)
}

fn traverse_tree(
    repo: &Repository,
    tree: &gix::objs::TreeRef<'_>,
    path_prefix: &str,
    filter: &FileFilter,
    curve_keys: &mut HashSet<CurveKey>,
    entries: &mut Vec<FileEntry>,
) -> Result<()> {
    for entry in tree.entries.iter() {
        let name = entry.filename.to_str().unwrap_or("");

        let path = if path_prefix.is_empty() {
            name.to_string()
        } else {
            format!("{}{}", path_prefix, name)
        };

        match entry.mode.kind() {
            EntryKind::Blob | EntryKind::BlobExecutable => {
                if !filter.matches(&path, name) {
                    continue;
                }

                curve_keys.insert(CurveKey::extension(file_extension(&path)));
                curve_keys.insert(CurveKey::directory(top_directory(&path)));

                entries.push(FileEntry {
                    path,
                    blob_id: entry.oid.to_owned(),
                });
            }
            EntryKind::Tree => {
                // Recursively traverse subdirectories
                let subtree_obj = repo.find_object(entry.oid)?;
                let subtree = subtree_obj.try_into_tree()?;
                let subtree_ref = subtree.decode()?;
                let new_prefix = format!("{}/", path);
                traverse_tree(repo, &subtree_ref, &new_prefix, filter, curve_keys, entries)?;
            }
            _ => {} // Skip links, commits, etc.
        }
    }
    Ok(())
}

fn file_extension(path: &str) -> String {
    let path_obj = Path::new(path);
    match path_obj.file_name().and_then(|name| name.to_str()) {
        Some(file_name) if !file_name.starts_with('.') => {
            match path_obj.extension().and_then(|ext| ext.to_str()) {
                Some(ext) => format!(".{ext}"),
                None => String::new(),
            }
        }
        _ => String::new(),
    }
}

fn top_directory(path: &str) -> String {
    match path.rsplit_once('/') {
        Some((prefix, _)) => match prefix.split('/').next() {
            Some(dir) if !dir.is_empty() => format!("{dir}/"),
            _ => "/".to_string(),
        },
        None => "/".to_string(),
    }
}

fn run_blame_for_entries(
    repo_path: &Path,
    commit_id: Oid,
    entries: &[FileEntry],
    commit2cohort: Arc<HashMap<Oid, String>>,
    ignore_whitespace: bool,
    use_mailmap: bool,
    jobs: usize,
    pool: Option<&ThreadPool>,
    timing: Arc<Mutex<TimingStats>>,
) -> Result<HashMap<String, HashMap<CurveKey, i64>>> {
    if entries.is_empty() {
        return Ok(HashMap::new());
    }

    let jobs = usize::max(1, jobs);

    let collected: Vec<Option<(String, HashMap<CurveKey, i64>)>> = if jobs <= 1 || pool.is_none() {
        let repo = gix::open(repo_path)
            .with_context(|| format!("Opening repository at {}", repo_path.display()))?;

        // Pre-load mailmap once (avoids repeated disk reads)
        let mailmap = if use_mailmap {
            Some(repo.open_mailmap())
        } else {
            None
        };

        let commit_graph = repo.commit_graph_if_enabled().ok().flatten().map(Arc::new);

        let context = WorkerContext {
            repo,
            ignore_whitespace,
            use_mailmap,
            mailmap,
            timing: Arc::clone(&timing),
            commit_graph,
        };

        entries
            .iter()
            .map(|entry| {
                blame_entry(
                    &context,
                    commit_id,
                    entry,
                    &commit2cohort,
                )
            })
            .collect::<Result<Vec<_>>>()?
    } else if let Some(pool) = pool {
        let repo_path_buf = repo_path.to_path_buf();
        pool.install(|| {
            entries
                .par_iter()
                .map_init(
                    || {
                        let repo = gix::open(&repo_path_buf).unwrap_or_else(|err| {
                            panic!(
                                "Failed to open repository {}: {err}",
                                repo_path_buf.display()
                            )
                        });

                        // Pre-load mailmap once per worker (avoids thousands of disk reads)
                        let mailmap = if use_mailmap {
                            Some(repo.open_mailmap())
                        } else {
                            None
                        };

                        let commit_graph = repo.commit_graph_if_enabled().ok().flatten().map(Arc::new);

                        WorkerContext {
                            repo,
                            ignore_whitespace,
                            use_mailmap,
                            mailmap,
                            timing: Arc::clone(&timing),
                            commit_graph,
                        }
                    },
                    |context, entry| {
                        blame_entry(
                            context,
                            commit_id,
                            entry,
                            &commit2cohort,
                        )
                    },
                )
                .collect::<Result<Vec<_>>>()
        })?
    } else {
        unreachable!();
    };

    let mut results = HashMap::new();
    for item in collected {
        if let Some((path, histogram)) = item {
            results.insert(path, histogram);
        }
    }
    Ok(results)
}

fn blame_entry(
    context: &WorkerContext,
    commit_id: Oid,
    entry: &FileEntry,
    commit2cohort: &HashMap<Oid, String>,
) -> Result<Option<(String, HashMap<CurveKey, i64>)>> {
    use gix::bstr::ByteSlice;

    // Convert file path to BString for gix API
    let file_path = BString::from(entry.path.as_bytes());

    // Time the blame operation
    let blame_start = Instant::now();

    // Use the Repository convenience method which handles the blame operation
    let blame_result = context.repo.blame_file(
        file_path.as_ref(),
        commit_id,
        gix::repository::blame_file::Options {
            follow_renames: true,
            ..Default::default()
        },
    );

    let blame = match blame_result {
        Ok(outcome) => outcome,
        // If blame fails, return empty histogram (same as git-of-theseus)
        Err(e) => {
            eprintln!("WARNING: Blame failed for {} (commit {}): {:?}",
                entry.path, commit_id.to_hex(), e);
            return Ok(Some((entry.path.clone(), HashMap::new())));
        }
    };
    let blame_elapsed = blame_start.elapsed();

    let ext_key = CurveKey::extension(file_extension(&entry.path));
    let dir_key = CurveKey::directory(top_directory(&entry.path));

    let mut histogram: HashMap<CurveKey, i64> = HashMap::new();

    // Time hunk processing
    let hunk_start = Instant::now();

    // Debug: track total lines
    let mut total_lines_in_file = 0i64;

    // Iterate through blame entries
    for (idx, blame_entry) in blame.entries.iter().enumerate() {
        let range = blame_entry.range_in_blamed_file();
        let lines = (range.end - range.start) as i64;
        total_lines_in_file += lines;
        let final_oid = blame_entry.commit_id;

        if entry.path.contains("minimal-typechecker.ts") && idx < 3 {
            eprintln!("  Entry {}: range {}..{} = {} lines, commit {}",
                idx, range.start, range.end, lines, final_oid.to_hex());
        }

        let cohort_label = commit2cohort
            .get(&final_oid)
            .cloned()
            .unwrap_or_else(|| {
                eprintln!("WARNING: Commit {} not in commit2cohort for file {}",
                    final_oid.to_hex(), entry.path);
                "MISSING".to_string()
            });
        accumulate(&mut histogram, CurveKey::cohort(cohort_label), lines);
        accumulate(&mut histogram, ext_key.clone(), lines);
        accumulate(&mut histogram, dir_key.clone(), lines);

        // Get author from the commit
        if let Ok(commit_obj) = context.repo.find_object(final_oid) {
            if let Ok(commit) = commit_obj.try_into_commit() {
                let (author, domain) = resolve_author(&commit, context.mailmap.as_ref());
                accumulate(&mut histogram, CurveKey::author(author), lines);
                accumulate(&mut histogram, CurveKey::domain(domain), lines);
            }
        }

        if commit2cohort.contains_key(&final_oid) {
            let sha_key = CurveKey::sha(oid_to_string(&final_oid));
            accumulate(&mut histogram, sha_key, lines);
        }
    }
    let hunk_elapsed = hunk_start.elapsed();

    // Debug output for files with discrepancies
    if total_lines_in_file == 0 && blame.entries.len() == 0 {
        eprintln!("WARNING: File {} has 0 lines blamed (0 blame entries, but blame succeeded)", entry.path);
    } else if total_lines_in_file == 0 && blame.entries.len() > 0 {
        eprintln!("WARNING: File {} has 0 lines blamed but {} entries!", entry.path, blame.entries.len());
    }
    if entry.path.contains("minimal-typechecker.ts") {
        eprintln!("DEBUG: minimal-typechecker.ts has {} lines blamed, {} blame entries",
            total_lines_in_file, blame.entries.len());
    }

    // Update timing stats
    if let Ok(mut timing) = context.timing.lock() {
        timing.blame_time += blame_elapsed;
        timing.hunk_processing_time += hunk_elapsed;
    }

    Ok(Some((entry.path.clone(), histogram)))
}

fn canonical_author(signature: &gix::actor::SignatureRef<'_>, mailmap: Option<&gix::mailmap::Snapshot>) -> (String, String) {
    let resolved = if let Some(map) = mailmap {
        map.resolve(*signature).to_owned()
    } else {
        signature.to_owned().expect("Failed to parse signature")
    };

    let name = resolved.name.to_string();
    let email = resolved.email.to_string();
    let domain = email.split('@').nth(1).unwrap_or("").to_string();
    (name, domain)
}

fn accumulate(map: &mut HashMap<CurveKey, i64>, key: CurveKey, delta: i64) {
    if delta == 0 {
        return;
    }
    *map.entry(key).or_insert(0) += delta;
}

fn update_commit_history(
    commit_history: &mut HashMap<String, Vec<(i64, i64)>>,
    cur_y: &HashMap<CurveKey, i64>,
    timestamp_secs: i64,
) {
    for (key, value) in cur_y {
        if key.kind() == CurveKeyKind::Sha {
            commit_history
                .entry(key.label().to_string())
                .or_default()
                .push((timestamp_secs, *value));
        }
    }
}

fn write_outputs(
    curve_keys: &HashSet<CurveKey>,
    curves: &HashMap<CurveKey, Vec<i64>>,
    ts: &[String],
    commit_history: &HashMap<String, Vec<(i64, i64)>>,
    config: &AnalyzeConfig,
) -> Result<()> {
    fs::create_dir_all(&config.outdir)
        .with_context(|| format!("Creating output directory {}", config.outdir.display()))?;

    write_curve_file(
        &config.outdir,
        "cohorts.json",
        CurveKeyKind::Cohort,
        curve_keys,
        curves,
        ts,
        |label| format!("Code added in {label}"),
    )?;
    write_curve_file(
        &config.outdir,
        "exts.json",
        CurveKeyKind::Ext,
        curve_keys,
        curves,
        ts,
        |label| label.to_string(),
    )?;
    write_curve_file(
        &config.outdir,
        "authors.json",
        CurveKeyKind::Author,
        curve_keys,
        curves,
        ts,
        |label| label.to_string(),
    )?;
    write_curve_file(
        &config.outdir,
        "dirs.json",
        CurveKeyKind::Dir,
        curve_keys,
        curves,
        ts,
        |label| label.to_string(),
    )?;
    write_curve_file(
        &config.outdir,
        "domains.json",
        CurveKeyKind::Domain,
        curve_keys,
        curves,
        ts,
        |label| label.to_string(),
    )?;

    write_survival_data(&config.outdir, commit_history)?;
    Ok(())
}

fn write_curve_file<F>(
    outdir: &Path,
    filename: &str,
    kind: CurveKeyKind,
    curve_keys: &HashSet<CurveKey>,
    curves: &HashMap<CurveKey, Vec<i64>>,
    ts: &[String],
    label_fn: F,
) -> Result<()>
where
    F: Fn(&str) -> String,
{
    let mut keys: Vec<CurveKey> = curve_keys
        .iter()
        .filter(|key| key.kind() == kind)
        .cloned()
        .collect();
    keys.sort_by(|a, b| a.label().cmp(b.label()));

    let y: Vec<Vec<i64>> = keys
        .iter()
        .map(|key| {
            curves
                .get(key)
                .cloned()
                .unwrap_or_else(|| vec![0; ts.len()])
        })
        .collect();
    let labels: Vec<String> = keys.iter().map(|key| label_fn(key.label())).collect();

    let output = json!({
        "y": y,
        "ts": ts,
        "labels": labels,
    });

    let mut file = File::create(outdir.join(filename))
        .with_context(|| format!("Writing {}", outdir.join(filename).display()))?;
    serde_json::to_writer_pretty(&mut file, &output)
        .with_context(|| format!("Serialising {}", outdir.join(filename).display()))?;
    Ok(())
}

fn write_survival_data(
    outdir: &Path,
    commit_history: &HashMap<String, Vec<(i64, i64)>>,
) -> Result<()> {
    let mut file = File::create(outdir.join("survival.json"))
        .with_context(|| format!("Writing {}", outdir.join("survival.json").display()))?;

    let mut map = serde_json::Map::new();
    for (sha, history) in commit_history {
        let points: Vec<_> = history.iter().map(|(t, c)| json!([t, c])).collect();
        map.insert(sha.clone(), json!(points));
    }

    serde_json::to_writer_pretty(&mut file, &serde_json::Value::Object(map))
        .context("Serialising survival data")
}

fn format_timestamp(seconds: i64) -> Result<String> {
    let dt = Utc
        .timestamp_opt(seconds, 0)
        .single()
        .ok_or_else(|| anyhow!("Invalid timestamp"))?;
    Ok(dt.format("%Y-%m-%dT%H:%M:%S").to_string())
}
